{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to target promotions with conversion prediction model to maximize Net Incremental Revenue?\n",
    "\n",
    "Every time a company chooses to promote some product through tactics like offering discounts or running a digital ad campaign, there is a certain cost as well as some potential revenue earning opportunity associated with it. If the company is not careful in choosing the right set of customers to be receiving the promotion, it can end up losing a lot of money without earning much in return.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset that I have used in this project was originally used as a take-home assignment provided by Starbucks for their job candidates. The data for this exercise consists of about 120,000 data points split in a 2:1 ratio among training and test files. In the experiment simulated by the data, an advertising promotion was tested to see if it would bring more customers to purchase a specific product priced at $10. Since it costs the company 0.15 to send out each promotion, it would be best to limit that promotion only to those that are most receptive to the promotion. Each data point includes one column indicating whether or not an individual was sent a promotion for the product, and one column indicating whether or not that individual eventually purchased that product. Each individual also has seven additional features associated with them, which are provided abstractly as V1-V7.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Our goal is to maximize the following metrics:\n",
    "\n",
    "* **Incremental Response Rate (IRR)** \n",
    "\n",
    "IRR depicts how many more customers purchased the product with the promotion, as compared to if they didn't receive the promotion. Mathematically, it's the ratio of the number of purchasers in the promotion group to the total number of customers in the purchasers group (_treatment_) minus the ratio of the number of purchasers in the non-promotional group to the total number of customers in the non-promotional group (_control_).\n",
    "\n",
    "$$ IRR = \\frac{purch_{treat}}{cust_{treat}} - \\frac{purch_{ctrl}}{cust_{ctrl}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ IRR =  (\\frac{N_{Treat\\_Purchase}}{N_{Treat}}) - (\\frac{N_{Control\\_Purchase}}{N_{Control}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Net Incremental Revenue (NIR)**\n",
    "\n",
    "NIR depicts how much is made (or lost) by sending out the promotion. Mathematically, this is 10 times the total number of purchasers that received the promotion minus 0.15 times the number of promotions sent out, minus 10 times the number of purchasers who were not given the promotion.\n",
    "\n",
    "$$ NIR = (10\\cdot purch_{treat} - 0.15 \\cdot cust_{treat}) - 10 \\cdot purch_{ctrl}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ NIR = (R_{Treat} \\cdot N_{Treat\\_Purchase} - C_{Treat} \\cdot N_{Treat}) - (R_{Control} \\cdot N_{Control\\_Purchase}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use the training data to understand what patterns in V1-V7 to indicate that a promotion should be provided to a user. Specifically, \n",
    "\n",
    "For a full description of what Starbucks provides to candidates see the [instructions available here](https://drive.google.com/open?id=18klca9Sef1Rs6q8DW4l7o349r8B70qXM).\n",
    "\n",
    "Below you can find the training data provided.  Explore the data and different optimization strategies.\n",
    "\n",
    "#### How To test our Strategy?\n",
    "\n",
    "When you feel like you have an optimization strategy, complete the `promotion_strategy` function to pass to the `test_results` function.  \n",
    "From past data, we know there are four possible outomes:\n",
    "\n",
    "Table of actual promotion vs. predicted promotion customers:  \n",
    "\n",
    "<table>\n",
    "<tr><th></th><th colspan = '2'>Actual</th></tr>\n",
    "<tr><th>Predicted</th><th>Yes</th><th>No</th></tr>\n",
    "<tr><th>Yes</th><td>I</td><td>II</td></tr>\n",
    "<tr><th>No</th><td>III</td><td>IV</td></tr>\n",
    "</table>\n",
    "\n",
    "The metrics are only being compared for the individuals we predict should obtain the promotion â€“ that is, quadrants I and II.  Since the first set of individuals that receive the promotion (in the training set) receive it randomly, we can expect that quadrants I and II will have approximately equivalent participants.  \n",
    "\n",
    "Comparing quadrant I to II then gives an idea of how well your promotion strategy will work in the future. \n",
    "\n",
    "Get started by reading in the data below.  See how each variable or combination of variables along with a promotion influences the chance of purchasing.  When you feel like you have a strategy for who should receive a promotion, test your strategy against the test dataset used in the final `test_results` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044332 -0.385883   1   1   2   2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in packages\n",
    "from itertools import combinations\n",
    "\n",
    "from test_results import test_results, score\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "%matplotlib inline\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt import space_eval\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    err = 1-f1_score(y_true, np.round(y_pred))\n",
    "    return 'f1_err', err\n",
    "import decimal\n",
    "from sklearn.model_selection import KFold\n",
    "def float_range(start, stop, step):\n",
    "    start = decimal.Decimal(start)\n",
    "    stop = decimal.Decimal(stop)\n",
    "    step = decimal.Decimal(step)\n",
    "    while (start < stop):\n",
    "        yield float(start)\n",
    "        start += step\n",
    "# load in the data\n",
    "train_data = pd.read_csv('./training.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>62970.972413</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>1.500662</td>\n",
       "      <td>29.973600</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>1.679608</td>\n",
       "      <td>2.327643</td>\n",
       "      <td>2.502898</td>\n",
       "      <td>1.701694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36418.440539</td>\n",
       "      <td>0.110234</td>\n",
       "      <td>0.868234</td>\n",
       "      <td>5.010626</td>\n",
       "      <td>1.000485</td>\n",
       "      <td>0.466630</td>\n",
       "      <td>0.841167</td>\n",
       "      <td>1.117349</td>\n",
       "      <td>0.457517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.104007</td>\n",
       "      <td>-1.684550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31467.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.591501</td>\n",
       "      <td>-0.905350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62827.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29.979744</td>\n",
       "      <td>-0.039572</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>94438.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>33.344593</td>\n",
       "      <td>0.826206</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>126184.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>50.375913</td>\n",
       "      <td>1.691984</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID      purchase            V1            V2            V3  \\\n",
       "count   84534.000000  84534.000000  84534.000000  84534.000000  84534.000000   \n",
       "mean    62970.972413      0.012303      1.500662     29.973600      0.000190   \n",
       "std     36418.440539      0.110234      0.868234      5.010626      1.000485   \n",
       "min         1.000000      0.000000      0.000000      7.104007     -1.684550   \n",
       "25%     31467.250000      0.000000      1.000000     26.591501     -0.905350   \n",
       "50%     62827.500000      0.000000      2.000000     29.979744     -0.039572   \n",
       "75%     94438.750000      0.000000      2.000000     33.344593      0.826206   \n",
       "max    126184.000000      1.000000      3.000000     50.375913      1.691984   \n",
       "\n",
       "                 V4            V5            V6            V7  \n",
       "count  84534.000000  84534.000000  84534.000000  84534.000000  \n",
       "mean       1.679608      2.327643      2.502898      1.701694  \n",
       "std        0.466630      0.841167      1.117349      0.457517  \n",
       "min        1.000000      1.000000      1.000000      1.000000  \n",
       "25%        1.000000      2.000000      2.000000      1.000000  \n",
       "50%        2.000000      2.000000      3.000000      2.000000  \n",
       "75%        2.000000      3.000000      4.000000      2.000000  \n",
       "max        2.000000      4.000000      4.000000      2.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cells for you to work and document as necessary - \n",
    "# definitely feel free to add more cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes    42364\n",
       "No     42170\n",
       "Name: Promotion, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"Promotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we treat the act of giving promotion as a treatment given by the company to its customers and those that were not given promotion as the control group, then we can see that there is nearly equal number of customers that belong to both the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for the distribution of values of target variable of our interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83494\n",
       "1     1040\n",
       "Name: purchase, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"purchase\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from this numbers that there is a high imbalance in the number of customers who chose to purchase the product vs those who didn't. We need to take care of this while using this dataset for trainin the machine learning algorithm by using some technique like oversampling from under represented (minority) value `1` for target variable `purchase`. **SMOTE** is one useful technique that generates balanced dataset for training purpose while also introducing some variations in the input variables while oversampling the data with minority target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1\n",
    "### Predicting if the customer will make the purchase only after receiving at the promotion.\n",
    "\n",
    "We are given dataset that includes customers that have been given and not given the promotion. As it costs the company 1.5$ to promote to the customer, it will try to avoid promoting it to the customers who are:\n",
    "- Not likely to purchase even after receiving the promotion\n",
    "- Are going to purchase even without receiving the promotion  \n",
    "\n",
    "Company is interested in giving the promotion to the customers who are likely to make the purchase only after receiving the promotion. The job of the preditive model is to predict whether the given customer falls into this category. If yes, then our algorithm will suggest the company to give promotion to that customer, otherwise it won't suggest to give promotion to that customer.\n",
    "\n",
    "A statistical model can be trained to decide whether to give customer the promotion or not by training it with dataset where each customer is labeled as 1 in the output variable if he has been shown promotion and has purchased to product, and 0 for the rest of the scenarios. We can name this new variable as `response` as it indicates whether the customer resonded positively to our promotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1[\"response\"] = (train_data_1[\"Promotion\"] == \"Yes\") & (train_data_1[\"purchase\"] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"V\"+str(x) for x in range(1,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data_1[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_data_1[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    83813\n",
       "True       721\n",
       "Name: response, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating balanced training dataset using Synthetic Minority Over-sampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42, ratio=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced_train, Y_balanced_train = sm.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting back to dataframe and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced_train = pd.DataFrame(X_balanced_train, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_balanced_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_balanced_train = pd.Series(Y_balanced_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computation time: 6.387 mins\n"
     ]
    }
   ],
   "source": [
    "cv = GridSearchCV(estimator=XGBClassifier(), param_grid={\n",
    "        \"max_depth\": range(5,8,1),\n",
    "        \"min_child_weight\": [5, 10, 20, 50],\n",
    "        \"gamma\": [0, 0.1, 0.2],\n",
    "        \"random_state\": [42],\n",
    "        \"n_estimators\": [1000]\n",
    "        },         \n",
    "        scoring=\"f1\", cv=3)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "fit_params= {\n",
    "            \"eval_set\": [(X_valid, Y_valid)],\n",
    "            \"eval_metric\": f1_eval,\n",
    "            \"early_stopping_rounds\":20,\n",
    "            \"verbose\": 0\n",
    "        }\n",
    "cv.fit(X_balanced_train, Y_balanced_train, **fit_params)\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.2,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 5,\n",
       " 'n_estimators': 1000,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.290235\tvalidation_0-f1_err:0.972261\n",
      "Multiple eval metrics have been passed: 'validation_0-f1_err' will be used for early stopping.\n",
      "\n",
      "Will train until validation_0-f1_err hasn't improved in 10 rounds.\n",
      "[10]\tvalidation_0-error:0.14497\tvalidation_0-f1_err:0.974553\n",
      "Stopping. Best iteration:\n",
      "[7]\tvalidation_0-error:0.170521\tvalidation_0-f1_err:0.971689\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.2,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=5, missing=None, n_estimators=1000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will help us deciding number of estimators\n",
    "xgb = XGBClassifier(n_estimators=1000)\n",
    "best_params_xgb = cv.best_params_\n",
    "xgb.set_params(**best_params_xgb)\n",
    "xgb.fit(X=X_balanced_train, y=Y_balanced_train.values.ravel(), eval_set=[(X_valid, Y_valid)], eval_metric=f1_eval, early_stopping_rounds=10, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_estimators = xgb.best_ntree_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found out optimal max_depth and number of estimators for XGBoost algorithm for our case. Train the XGBoost on entire training dataset for using it in promotion strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced, Y_balanced = sm.fit_sample(X,Y)\n",
    "X_balanced = pd.DataFrame(X_balanced, columns=features)\n",
    "Y_balanced = pd.Series(Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.2,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=7,\n",
       "              min_child_weight=5, missing=None, n_estimators=8, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=best_params_xgb[\"max_depth\"],\n",
    "                    gamma=best_params_xgb[\"gamma\"],\n",
    "                    min_child_weight=best_params_xgb[\"min_child_weight\"],\n",
    "                    n_estimators=optimal_n_estimators,\n",
    "                    random_state=42)\n",
    "xgb.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb, open(data_dir + '/xgb_best_approach_1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(data_dir + \"/xgb_best_approach_1.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_strategy(df):\n",
    "    '''\n",
    "    INPUT \n",
    "    df - a dataframe with *only* the columns V1 - V7 (same as train_data)\n",
    "\n",
    "    OUTPUT\n",
    "    promotion_df - np.array with the values\n",
    "                   'Yes' or 'No' related to whether or not an \n",
    "                   individual should recieve a promotion \n",
    "                   should be the length of df.shape[0]\n",
    "                \n",
    "    Ex:\n",
    "    INPUT: df\n",
    "    \n",
    "    V1\tV2\t  V3\tV4\tV5\tV6\tV7\n",
    "    2\t30\t-1.1\t1\t1\t3\t2\n",
    "    3\t32\t-0.6\t2\t3\t2\t2\n",
    "    2\t30\t0.13\t1\t1\t4\t2\n",
    "    \n",
    "    OUTPUT: promotion\n",
    "    \n",
    "    array(['Yes', 'Yes', 'No'])\n",
    "    indicating the first two users would recieve the promotion and \n",
    "    the last should not.\n",
    "    '''\n",
    "    \n",
    "    test = df\n",
    "    \n",
    "    preds = model.predict(test)\n",
    "    promotion = []\n",
    "    for pred in preds:\n",
    "        if pred:\n",
    "            promotion.append('Yes')\n",
    "        else:\n",
    "            promotion.append('No')\n",
    "    promotion = np.array(promotion)\n",
    "    \n",
    "    \n",
    "    return promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  See how well your strategy worked on our test data below!\n",
      "\n",
      "Your irr with this strategy is 0.0206.\n",
      "\n",
      "Your nir with this strategy is 259.25.\n",
      "We came up with a model with an irr of 0.0188 and an nir of 189.45 on the test set.\n",
      "\n",
      " How did you do?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020606371512773836, 259.25)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results(promotion_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Indicating whether a person has received promotion as input variable and training a single model to predict whether the person will make purchase or not. \n",
    "- For the purpose of deciding whether to send promotion to the person, we can first calculate the probability of person making purchase after receiving promotion and without receiving promotion by senting `promotion` input variable as 1 or 0 respectively and calculating the difference between the two probabailities. If the difference turns out to be greater than some threshold value, in that case we can send promotion to the person. \n",
    "- The value of threshold can be decided using Hyper Parameter optimization technique while using NIR formula for calcualting the return value of the objective function to be minimized i.e. the score can be -NIR.\n",
    "- Here I have used **log loss as evaluation metric** while tuning hyper parameters of XGBoost as I am more interested in calculating accurate probabilities of person giving certain response than just predicting right response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1[\"response\"] = train_data_1[\"purchase\"] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_1[\"response\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"V\"+str(x) for x in range(1,8)] + [\"Promotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.concat([train_data_1[features],pd.get_dummies(train_data_1[\"Promotion\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(train_data_1[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84534, 9)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_data_1[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    83494\n",
       "True      1040\n",
       "Name: response, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating balanced training dataset using Synthetic Minority Over-sampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42, ratio=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced_train, Y_balanced_train = sm.fit_resample(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting back to dataframe and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced_train = pd.DataFrame(X_balanced_train, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'Promotion_No',\n",
       "       'Promotion_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_balanced_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_balanced_train = pd.Series(Y_balanced_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed computation time: 144.219 mins\n"
     ]
    }
   ],
   "source": [
    "cv = GridSearchCV(estimator=XGBClassifier(), param_grid={\n",
    "        \"max_depth\": range(5,8,1),\n",
    "        \"min_child_weight\": [5, 10, 20, 50],\n",
    "        \"gamma\": [0, 0.1, 0.2],\n",
    "        \"random_state\": [42],\n",
    "        \"n_estimators\": [1000]\n",
    "        },\n",
    "        scoring=\"f1\",\n",
    "         cv=3)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "fit_params= {\n",
    "            \"eval_set\": [(X_valid, Y_valid)],\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            \"early_stopping_rounds\":20,\n",
    "            \"verbose\": 0\n",
    "        }\n",
    "cv.fit(X_balanced_train, Y_balanced_train, **fit_params)\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.1,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 10,\n",
       " 'n_estimators': 1000,\n",
       " 'random_state': 42}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.651736\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[10]\tvalidation_0-logloss:0.467548\n",
      "[20]\tvalidation_0-logloss:0.361545\n",
      "[30]\tvalidation_0-logloss:0.298454\n",
      "[40]\tvalidation_0-logloss:0.25701\n",
      "[50]\tvalidation_0-logloss:0.230457\n",
      "[60]\tvalidation_0-logloss:0.211893\n",
      "[70]\tvalidation_0-logloss:0.19325\n",
      "[80]\tvalidation_0-logloss:0.179203\n",
      "[90]\tvalidation_0-logloss:0.163888\n",
      "[100]\tvalidation_0-logloss:0.152161\n",
      "[110]\tvalidation_0-logloss:0.139522\n",
      "[120]\tvalidation_0-logloss:0.132642\n",
      "[130]\tvalidation_0-logloss:0.124634\n",
      "[140]\tvalidation_0-logloss:0.117031\n",
      "[150]\tvalidation_0-logloss:0.110356\n",
      "[160]\tvalidation_0-logloss:0.104467\n",
      "[170]\tvalidation_0-logloss:0.100311\n",
      "[180]\tvalidation_0-logloss:0.094953\n",
      "[190]\tvalidation_0-logloss:0.090794\n",
      "[200]\tvalidation_0-logloss:0.088276\n",
      "[210]\tvalidation_0-logloss:0.085795\n",
      "[220]\tvalidation_0-logloss:0.084368\n",
      "[230]\tvalidation_0-logloss:0.082699\n",
      "[240]\tvalidation_0-logloss:0.080333\n",
      "[250]\tvalidation_0-logloss:0.079036\n",
      "[260]\tvalidation_0-logloss:0.077853\n",
      "[270]\tvalidation_0-logloss:0.076522\n",
      "[280]\tvalidation_0-logloss:0.075776\n",
      "[290]\tvalidation_0-logloss:0.075155\n",
      "[300]\tvalidation_0-logloss:0.074299\n",
      "[310]\tvalidation_0-logloss:0.073751\n",
      "[320]\tvalidation_0-logloss:0.073337\n",
      "[330]\tvalidation_0-logloss:0.073036\n",
      "[340]\tvalidation_0-logloss:0.072558\n",
      "[350]\tvalidation_0-logloss:0.072308\n",
      "[360]\tvalidation_0-logloss:0.072077\n",
      "[370]\tvalidation_0-logloss:0.071781\n",
      "[380]\tvalidation_0-logloss:0.07163\n",
      "[390]\tvalidation_0-logloss:0.071283\n",
      "[400]\tvalidation_0-logloss:0.071203\n",
      "[410]\tvalidation_0-logloss:0.071086\n",
      "[420]\tvalidation_0-logloss:0.07099\n",
      "[430]\tvalidation_0-logloss:0.070879\n",
      "[440]\tvalidation_0-logloss:0.070729\n",
      "[450]\tvalidation_0-logloss:0.070651\n",
      "[460]\tvalidation_0-logloss:0.070625\n",
      "[470]\tvalidation_0-logloss:0.070538\n",
      "[480]\tvalidation_0-logloss:0.070495\n",
      "[490]\tvalidation_0-logloss:0.070458\n",
      "[500]\tvalidation_0-logloss:0.070361\n",
      "[510]\tvalidation_0-logloss:0.070321\n",
      "[520]\tvalidation_0-logloss:0.070278\n",
      "[530]\tvalidation_0-logloss:0.070228\n",
      "Stopping. Best iteration:\n",
      "[527]\tvalidation_0-logloss:0.070218\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=10, missing=None, n_estimators=1000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will help us deciding number of estimators\n",
    "xgb = XGBClassifier(n_estimators=1000)\n",
    "best_params_xgb = cv.best_params_\n",
    "xgb.set_params(**best_params_xgb)\n",
    "xgb.fit(X=X_balanced_train, y=Y_balanced_train.values.ravel(), eval_set=[(X_valid, Y_valid)], eval_metric=\"logloss\", early_stopping_rounds=10, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_n_estimators = xgb.best_ntree_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found out optimal max_depth and number of estimators for XGBoost algorithm for our case. Train the XGBoost on entire training dataset for using it in promotion strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced, Y_balanced = sm.fit_sample(X,Y)\n",
    "X_balanced = pd.DataFrame(X_balanced, columns=features)\n",
    "Y_balanced = pd.Series(Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=10, missing=None, n_estimators=528, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=best_params_xgb[\"max_depth\"],\n",
    "                    gamma=best_params_xgb[\"gamma\"],\n",
    "                    min_child_weight=best_params_xgb[\"min_child_weight\"],\n",
    "                    n_estimators=optimal_n_estimators,\n",
    "                    random_state=42)\n",
    "xgb.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb, open(data_dir + '/xgb_best_approach_2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(data_dir + \"/xgb_best_approach_2.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `diff` as difference in the probabilities of person purchasing the product with and without receiving promotion. We have to choose the threshold value and if the `diff` is higher than that threshold value than we can choose to show promotion to that person. To decide the value of threshold that maximizes the NIR for given prediction model, I evaluate the thresholds in range from 0 to 0.1 by calculating the mean of NIR on 10 folds of the validation dataset and choose threshold value with maximum NIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X, Y, diff_threshold, after_promotion_purchase_prob_threshold):\n",
    "    def score(df, promo_pred_col = 'Promotion'):\n",
    "        n_treat       = df.loc[df[promo_pred_col] == 'Yes',:].shape[0]\n",
    "        n_control     = df.loc[df[promo_pred_col] == 'No',:].shape[0]\n",
    "        n_treat_purch = df.loc[df[promo_pred_col] == 'Yes', 'purchase'].sum()\n",
    "        n_ctrl_purch  = df.loc[df[promo_pred_col] == 'No', 'purchase'].sum()\n",
    "        nir = 10 * n_treat_purch - 0.15 * n_treat - 10 * n_ctrl_purch\n",
    "        return nir\n",
    "    \n",
    "    nir_scores = []\n",
    "    kf = KFold(n_splits=10, random_state=42)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_valid = X.loc[train_index], X.loc[test_index]\n",
    "        Y_train, Y_valid = Y.loc[train_index], Y.loc[test_index]\n",
    "        \n",
    "        # As we have already trained the hyper parameters for XGBoost, we need not train it again here\n",
    "        # we can use the trained model, to calculate score for given threshold value\n",
    "        model = pickle.load(open(data_dir + \"/xgb_best_approach_2.pkl\", 'rb'))\n",
    "        \n",
    "        X_valid_with_promo = X_valid.copy()\n",
    "        # predict probability of purchase with promotion\n",
    "        X_valid_with_promo[\"Promotion_Yes\"] = 1\n",
    "        X_valid_with_promo[\"Promotion_No\"] = 0\n",
    "        probs_with_promotion = model.predict_proba(X_valid_with_promo)[:, 1]\n",
    "\n",
    "        # predict probability of purchase without promotion\n",
    "        X_valid_with_promo[\"Promotion_Yes\"] = 0\n",
    "        X_valid_with_promo[\"Promotion_No\"] = 1\n",
    "\n",
    "        probs_without_promotion = model.predict_proba(X_valid_with_promo)[:, 1]\n",
    "\n",
    "        # calculate the difference as diff\n",
    "        diff = probs_with_promotion - probs_without_promotion\n",
    "\n",
    "        # if diff is above threshold choose to promote else don't\n",
    "        promos = (probs_with_promotion > after_promotion_purchase_prob_threshold) & (diff > diff_threshold)\n",
    "        val_data = X_valid.copy()\n",
    "        val_data[\"Promotion\"] = \"No\"\n",
    "        val_data.loc[val_data[\"Promotion_Yes\"] == 1, \"Promotion\"] = \"Yes\"\n",
    "        val_data[\"purchase\"] = Y_valid.copy()\n",
    "        score_df = val_data.iloc[np.where(promos)]\n",
    "        nir = score(score_df)\n",
    "        nir_scores.append(nir)\n",
    "    return np.asscalar(np.mean(nir_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_valid.index == Y_valid.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.037}\n",
      "  0%|          | 0/200 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  del sys.path[0]\n",
      "\n",
      "/Users/atharva/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nir: 0.985                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.037}\n",
      "nir: 2.91                                                         \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.02}\n",
      "nir: 13.084999999999999                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.031}      \n",
      "nir: 2.91                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.031}      \n",
      "nir: 0.0                                                                       \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.027}      \n",
      "nir: 13.01                                                                     \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.023}      \n",
      "nir: 0.985                                                                     \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.026}      \n",
      "nir: 2.91                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.02}       \n",
      "nir: 1.97                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.031}      \n",
      "nir: 12.725                                                                    \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.025}       \n",
      "nir: 12.51                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.038}       \n",
      "nir: 1.97                                                                       \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.021}       \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.039}       \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.039}       \n",
      "nir: 1.97                                                                       \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.028}       \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.027}       \n",
      "nir: 0.985                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.028}       \n",
      "nir: 1.97                                                                       \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.036}       \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.03}        \n",
      "nir: 13.01                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.022}       \n",
      "nir: 13.01                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.033}       \n",
      "nir: 0.985                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.03}        \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.034}       \n",
      "nir: 13.219999999999999                                                         \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.029}       \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.024}       \n",
      "nir: 12.315000000000001                                                         \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.034}       \n",
      "nir: 0.985                                                                      \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.035}       \n",
      "nir: 0.0                                                                        \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.032}       \n",
      "nir: 12.830000000000002                                                         \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.037}       \n",
      "nir: 14.535                                                                     \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.037}       \n",
      "nir: 1.97                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.023}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.026}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.025}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.038}\n",
      "nir: 12.715                                                         \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.031}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.021}\n",
      "nir: 12.430000000000001                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.032}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.029}\n",
      "nir: 1.97                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.036}\n",
      "nir: 2.91                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.027}\n",
      "nir: 13.945000000000002                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.033}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.035}\n",
      "nir: 1.97                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.024}\n",
      "nir: 13.01                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.037}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.023}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.026}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.025}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.03}\n",
      "nir: 13.560000000000002                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.039}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.036}\n",
      "nir: 1.97                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.027}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.037}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.033}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.029}\n",
      "nir: 1.97                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.032}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.035}\n",
      "nir: 13.325                                                         \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.037}\n",
      "nir: 13.01                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.028}\n",
      "nir: 14.184999999999999                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.03}\n",
      "nir: 2.91                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.037}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.022}\n",
      "nir: 12.805000000000001                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.039}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.026}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.02}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.025}\n",
      "nir: 1.97                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.023}\n",
      "nir: 13.0                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.038}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.036}\n",
      "nir: 13.01                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.027}\n",
      "nir: 2.91                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.034}\n",
      "nir: 0.0                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.037}\n",
      "nir: 0.985                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.029}\n",
      "nir: 13.409999999999997                                              \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.024}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.035}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.028}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.03}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.021}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.034}\n",
      "nir: 13.01                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.036}\n",
      "nir: 13.415000000000001                                              \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.029}\n",
      "nir: 2.91                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.032}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.024}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.03}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.026}\n",
      "nir: 13.01                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.039}\n",
      "nir: 2.91                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.038}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.021}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.036}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.033}\n",
      "nir: 12.934999999999999                                              \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.032}\n",
      "nir: 13.01                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.2, 'diff_threshold': 0.024}\n",
      "nir: 2.91                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.022}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.03}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.02}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.1, 'diff_threshold': 0.031}\n",
      "nir: 13.01                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.026}\n",
      "nir: 13.825                                                          \n",
      "{'after_promotion_purchase_prob_threshold': 0.9, 'diff_threshold': 0.027}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.5, 'diff_threshold': 0.029}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.034}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.033}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.032}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.024}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.8, 'diff_threshold': 0.025}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.7, 'diff_threshold': 0.038}\n",
      "nir: 0.0                                                             \n",
      "{'after_promotion_purchase_prob_threshold': 0.3, 'diff_threshold': 0.02}\n",
      "nir: 1.97                                                            \n",
      "{'after_promotion_purchase_prob_threshold': 0.6, 'diff_threshold': 0.021}\n",
      "nir: 0.985                                                           \n",
      "{'after_promotion_purchase_prob_threshold': 0.4, 'diff_threshold': 0.031}\n",
      "nir: 1.97                                                            \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:52<00:00,  1.78it/s, best loss: -14.535]\n",
      "{'after_promotion_purchase_prob_threshold': 0.0, 'diff_threshold': 0.037}\n",
      "Elapsed computation time: 1.872 mins\n"
     ]
    }
   ],
   "source": [
    "evaluated_point_scores = {}\n",
    "\n",
    "def objective_threshold(params):\n",
    "    if (str(params) in evaluated_point_scores):\n",
    "        return evaluated_point_scores[str(params)]\n",
    "    else:\n",
    "        print(params)\n",
    "        diff_threshold = params[\"diff_threshold\"]\n",
    "        after_promotion_purchase_prob_threshold = params[\"after_promotion_purchase_prob_threshold\"]\n",
    "        nir_score = evaluate(X=X_valid, Y=Y_valid, \n",
    "                             diff_threshold=diff_threshold, \n",
    "                             after_promotion_purchase_prob_threshold=after_promotion_purchase_prob_threshold)\n",
    "        print(\"nir: \" + str(nir_score))        \n",
    "        evaluated_point_scores[str(params)] = -nir_score\n",
    "        return -nir_score\n",
    "\n",
    "param_space = {\n",
    "    \"diff_threshold\": hp.choice(\"diff_threshold\", list(float_range(\"0.02\", \"0.04\", \"0.001\"))),\n",
    "    \"after_promotion_purchase_prob_threshold\": hp.choice(\"after_promotion_purchase_prob_threshold\", list(float_range(\"0.0\", \"1.0\", \"0.1\")))\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "best_params_threshold = space_eval(\n",
    "    param_space, \n",
    "    fmin(objective_threshold, \n",
    "         param_space, \n",
    "         algo=hyperopt.tpe.suggest,\n",
    "         max_evals=200))\n",
    "print(best_params_threshold)\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print('Elapsed computation time: {:.3f} mins'.format(elapsed_time))\n",
    "best_diff_threshold = best_params_threshold[\"diff_threshold\"]\n",
    "best_after_promotion_purchase_prob_threshold = best_params_threshold[\"after_promotion_purchase_prob_threshold\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promotion_strategy(df):\n",
    "    '''\n",
    "    INPUT \n",
    "    df - a dataframe with *only* the columns V1 - V7 (same as train_data)\n",
    "\n",
    "    OUTPUT\n",
    "    promotion_df - np.array with the values\n",
    "                   'Yes' or 'No' related to whether or not an \n",
    "                   individual should recieve a promotion \n",
    "                   should be the length of df.shape[0]\n",
    "                \n",
    "    Ex:\n",
    "    INPUT: df\n",
    "    \n",
    "    V1\tV2\t  V3\tV4\tV5\tV6\tV7\n",
    "    2\t30\t-1.1\t1\t1\t3\t2\n",
    "    3\t32\t-0.6\t2\t3\t2\t2\n",
    "    2\t30\t0.13\t1\t1\t4\t2\n",
    "    \n",
    "    OUTPUT: promotion\n",
    "    \n",
    "    array(['Yes', 'Yes', 'No'])\n",
    "    indicating the first two users would recieve the promotion and \n",
    "    the last should not.\n",
    "    '''\n",
    "    X = df.copy()\n",
    "    # predict probability of purchase with promotion\n",
    "\n",
    "    X[\"Promotion_No\"] = 0\n",
    "    X[\"Promotion_Yes\"] = 1\n",
    "    probs_with_promotion = model.predict_proba(X)[:, 1]\n",
    "\n",
    "\n",
    "    # predict probability of purchase without promotion\n",
    "    \n",
    "    X[\"Promotion_No\"] = 1\n",
    "    X[\"Promotion_Yes\"] = 0\n",
    "    probs_without_promotion = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # calculate the difference as diff\n",
    "    diff = probs_with_promotion - probs_without_promotion        \n",
    "\n",
    "    should_promote = pd.DataFrame() \n",
    "    should_promote[\"promo\"] = (probs_with_promotion > best_after_promotion_purchase_prob_threshold) & (diff > best_diff_threshold)\n",
    "    \n",
    "    should_promote.loc[diff >= best_diff_threshold, \"promo\"] = \"Yes\"\n",
    "    should_promote.loc[diff < best_diff_threshold, \"promo\"] = \"No\"    \n",
    "    return should_promote[\"promo\"].to_numpy(dtype=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  See how well your strategy worked on our test data below!\n",
      "\n",
      "Your irr with this strategy is 0.0188.\n",
      "\n",
      "Your nir with this strategy is 98.30.\n",
      "We came up with a model with an irr of 0.0188 and an nir of 189.45 on the test set.\n",
      "\n",
      " How did you do?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.018826981638688733, 98.30000000000001)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results(promotion_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 3 (Can be tried on this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try the two models approach that is commonly recommended on literature related to uplift measurement. In this approach, we create one model for people who have received the promotion and another model for those who haven't received it. Each model predicts whether the person would purchase the product. The difference between the probability predicted by first model and second model is to be considered for deciding wether to promote to to that person or not. \n",
    "\n",
    "Caveat here is that the error of prediction can get doubled as we are using two separate models. Also the scale of the probabilities predicted by two models may not be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
